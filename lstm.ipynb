{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata \n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "import unicodedata \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 0])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_data = pd.read_table(\"train.tsv\")\n",
    "movies_data.shape\n",
    "movies_data[\"Sentiment\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292, 3)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_test = pd.read_table(\"test.tsv\")\n",
    "movie_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies_data['Phrase'] = (movies_data['Phrase'].str.lower()\n",
    "              .str.decode('utf-8')\n",
    "              .map(lambda x: unicodedata.normalize('NFKD', x))\n",
    "              .str.encode('ascii', 'ignore'))\n",
    "\n",
    "movie_test['Phrase'] = (movie_test['Phrase'].str.lower()\n",
    "              .str.decode('utf-8')\n",
    "              .map(lambda x: unicodedata.normalize('NFKD', x))\n",
    "              .str.encode('ascii', 'ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [\"Phrase\", \"Sentiment\"]\n",
    "movies_data_new =movies_data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_data_new.to_csv(\"train.csv\", index= False)\n",
    "movie_test.to_csv(\"test.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66293\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"test.csv\", 'r') as file: \n",
    "    test = list(csv.reader(file))\n",
    "\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"train.csv\", 'r') as file: \n",
    "    reviews = list(csv.reader(file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(reviews, score):\n",
    "    # Join together the text in the reviews for a particular tone.\n",
    "    #We lowercase to avoid \"Not\" and \"not\" being seen as different words, for example.\n",
    "    return \" \".join([r[0].lower() for r in reviews if r[1] == str(score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_text = get_text(reviews, 0)\n",
    "somewhat_negative_text = get_text(reviews, 1)\n",
    "neutral_text = get_text(reviews, 2)\n",
    "somewhat_positive_text = get_text(reviews, 3)\n",
    "positive_text = get_text(reviews, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stop_words(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stop])\n",
    "    return text\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    s = ''.join([i for i in s if i not in set(string.punctuation)])\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_text = stop_words(negative_text)\n",
    "#negative_text = remove_punctuation(negative_text)\n",
    "\n",
    "positive_text = stop_words(positive_text)\n",
    "#positive_text = remove_punctuation (positive_text)\n",
    "\n",
    "somewhat_negative_text = stop_words(somewhat_negative_text)\n",
    "#somewhat_negative_text = remove_punctuation(somewhat_negative_text)\n",
    "\n",
    "somewhat_positive_text = stop_words(somewhat_positive_text)\n",
    "#somewhat_positive_text = remove_punctuation (somewhat_positive_text)\n",
    "\n",
    "neutral_text = stop_words(neutral_text)\n",
    "#neutral_text = remove_punctuation (neutral_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"positive_text.txt\", \"w\") as text_file:\n",
    "    text_file.write(positive_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"negative_text.txt\", \"w\") as text_file:\n",
    "    text_file.write(negative_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"somewhat_negative_text.txt\", \"w\") as text_file:\n",
    "    text_file.write(somewhat_negative_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"neutral_text.txt\", \"w\") as text_file:\n",
    "    text_file.write(neutral_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"somewhat_positive_text.txt\", \"w\") as text_file:\n",
    "    text_file.write(neutral_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "np.random.seed(1337)  # For Reproducibility\n",
    "import multiprocessing\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from utils import *\n",
    "import glob\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import unicodedata\n",
    "from joblib import Parallel, delayed\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import io\n",
    "    sentences_pos = []\n",
    "    ff = \"positive_text.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_pos.append(line)\n",
    "    sentences_neg = []\n",
    "    ff = \"negative_text.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_neg.append(line)\n",
    "    \n",
    "    sentences_smwn = []\n",
    "    ff = \"somewhat_negative_text.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_smwn.append(line)\n",
    "            \n",
    "    sentences_smwp = []    \n",
    "    ff = \"somewhat_positive_text.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_smwp.append(line)\n",
    "            \n",
    "    sentences_neutral = []        \n",
    "    ff = \"neutral_text.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_neutral.append(line)\n",
    "            \n",
    "            \n",
    "    X = sentences_pos+sentences_neg\n",
    "    y = [4]*len(sentences_pos)+[0]*len(sentences_neg)+[1]*len(sentences_smwn)+[2]*len(sentences_neutral) + [3]*len(sentences_smwp)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 0, 1, 2, 3]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_data()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stemmering_sentences(sentence):\n",
    "    # Remove punctuation, stopword and then stemmering\n",
    "    punctuation = set(string.punctuation)\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    # tmp = unicode(sentence, errors='ignore')\n",
    "    tmp = sentence\n",
    "    doc = [stemmer.stem(word.lower()) for word in nltk.word_tokenize(tmp) if\n",
    "           (word not in punctuation) and (word not in nltk.corpus.stopwords.words('english')) and (word != 'br')]\n",
    "    return doc\n",
    "\n",
    "def stemmering_sentences_mrd(X, y):\n",
    "    sentences_stem = Parallel(n_jobs=4)(delayed(stemmering_sentences)(sentence) for sentence in tqdm.tqdm(X, desc=\"stem\"))\n",
    "    return sentences_stem, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stem: 100%|██████████| 2/2 [00:00<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = stemmering_sentences_mrd(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set parameters:\n",
    "vocab_dim = 20\n",
    "maxlen = 100\n",
    "n_iterations = 1  # ideally more..\n",
    "# Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. \n",
    "n_exposures = 3\n",
    "window_size = 5\n",
    "batch_size = 32\n",
    "n_epoch = 2\n",
    "input_length = 100\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine_train_test_X = terms_by_doc_train + terms_by_doc_test\n",
    "combine_train_test_X = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Word2vec model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training a Word2vec model...')\n",
    "model = Word2Vec(size=vocab_dim,\n",
    "                 min_count=n_exposures,\n",
    "                 window=window_size,\n",
    "                 workers=cpu_count,\n",
    "                 iter=n_iterations)\n",
    "model.build_vocab(combine_train_test_X)\n",
    "model.train(combine_train_test_X)\n",
    "#print len(model.vocab.viewkeys()) -> 11403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Arrays for Keras Embedding Layer...\n"
     ]
    }
   ],
   "source": [
    "gensim_dict = Dictionary()\n",
    "gensim_dict.doc2bow(model.vocab.keys(), allow_update=True)\n",
    "# gensim_dict.items() returns [(0, u\"'surpris\"), (1, u'woodi'), (2, u'yellow'),...]\n",
    "# K+1 aims at avoiding 0 as index.\n",
    "w2indx = {v: k+1 for k, v in gensim_dict.items()}\n",
    "w2vec = {word: model[word] for word in w2indx.keys()}\n",
    "# print len(model[\"surpris\"]) -> 100\n",
    "print('Setting up Arrays for Keras Embedding Layer...')\n",
    "n_symbols = len(w2indx) + 1  # adding 1 to account for 0th index\n",
    "embedding_weights = np.zeros((n_symbols + 1, vocab_dim))\n",
    "for word, index in w2indx.items():\n",
    "    embedding_weights[index, :] = w2vec[word]\n",
    "# print embedding_weights.shape -> (11405, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-5f8e58de6842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mterms_by_doc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterms_by_doc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m58\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterms_by_doc_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnew_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "terms_by_doc_train, terms_by_doc_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=58)\n",
    "X_train = []\n",
    "for doc in terms_by_doc_train:\n",
    "    new_txt = []\n",
    "    for word in doc:\n",
    "        try:\n",
    "            new_txt.append(w2indx[word])\n",
    "        except:\n",
    "            new_txt.append(0)\n",
    "    X_train.append(new_txt)\n",
    "X_test = []\n",
    "for doc in terms_by_doc_test:\n",
    "    new_txt = []\n",
    "    for word in doc:\n",
    "        try:\n",
    "            new_txt.append(w2indx[word])\n",
    "        except:\n",
    "            new_txt.append(0)\n",
    "    X_test.append(new_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
