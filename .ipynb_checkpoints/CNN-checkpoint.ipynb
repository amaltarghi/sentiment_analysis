{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "np.random.seed(1337)  # For Reproducibility\n",
    "import multiprocessing\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from utils import *\n",
    "import glob\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import unicodedata\n",
    "from joblib import Parallel, delayed\n",
    "import tqdm\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import io\n",
    "    sentences_pos = []\n",
    "    ff = \"positive_text.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_pos.append(line)\n",
    "    sentences_neg = []\n",
    "    ff = \"negative_text.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_neg.append(line)\n",
    "    \n",
    "    sentences_smwn = []\n",
    "    ff = \"somewhat_negative_text.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_smwn.append(line)\n",
    "            \n",
    "    sentences_smwp = []    \n",
    "    ff = \"somewhat_positive_text.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_smwp.append(line)\n",
    "            \n",
    "    sentences_neutral = []        \n",
    "    ff = \"neutral_text.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_neutral.append(line)\n",
    "            \n",
    "            \n",
    "    X = sentences_pos+sentences_neg+sentences_smwn+sentences_neutral+sentences_smwp\n",
    "    y = [4]*len(sentences_pos)+[0]*len(sentences_neg)+[1]*len(sentences_smwn)+[2]*len(sentences_neutral) + [3]*len(sentences_smwp)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemmering_sentences(sentence):\n",
    "    # Remove punctuation, stopword and then stemmering\n",
    "    punctuation = set(string.punctuation)\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "\n",
    "    # tmp = unicode(sentence, errors='ignore')\n",
    "    tmp = sentence\n",
    "    doc = [stemmer.stem(word.lower()) for word in nltk.word_tokenize(tmp) if\n",
    "           (word not in punctuation) and (word not in nltk.corpus.stopwords.words('english')) and (word != 'br')]\n",
    "    return doc\n",
    "\n",
    "def stemming(X, y):\n",
    "    sentences_stem = Parallel(n_jobs=4)(delayed(stemmering_sentences)(sentence) for sentence in tqdm.tqdm(X, desc=\"stem\"))\n",
    "    return sentences_stem, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stem: 100%|██████████| 202715/202715 [02:24<00:00, 1398.27it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = stemming(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "vocab_dim = 10\n",
    "maxlen = 10\n",
    "n_iterations = 10  # ideally more..\n",
    "# Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. \n",
    "n_exposures = 2\n",
    "window_size = 5\n",
    "batch_size = 128\n",
    "nb_filter = 500\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 5\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine_train_test_X = terms_by_doc_train + terms_by_doc_test\n",
    "combine_train_test_X = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Word2vec model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6903886"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training a Word2vec model...')\n",
    "model = Word2Vec(size=vocab_dim,\n",
    "                 min_count=n_exposures,\n",
    "                 window=window_size,\n",
    "                 workers=cpu_count,\n",
    "                 iter=n_iterations)\n",
    "model.build_vocab(combine_train_test_X)\n",
    "model.train(combine_train_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Arrays for Keras Embedding Layer...\n"
     ]
    }
   ],
   "source": [
    "gensim_dict = Dictionary()\n",
    "gensim_dict.doc2bow(model.vocab.keys(), allow_update=True)\n",
    "# gensim_dict.items() returns [(0, u\"'surpris\"), (1, u'woodi'), (2, u'yellow'),...]\n",
    "# K+1 aims at avoiding 0 as index.\n",
    "w2indx = {v: k+1 for k, v in gensim_dict.items()}\n",
    "w2vec = {word: model[word] for word in w2indx.keys()}\n",
    "# print len(model[\"surpris\"]) -> 100\n",
    "print('Setting up Arrays for Keras Embedding Layer...')\n",
    "n_symbols = len(w2indx) + 1  # adding 1 to account for 0th index\n",
    "embedding_weights = np.zeros((n_symbols + 1, vocab_dim))\n",
    "for word, index in w2indx.items():\n",
    "    embedding_weights[index, :] = w2vec[word]\n",
    "# print embedding_weights.shape -> (11405, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "terms_by_doc_train, terms_by_doc_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=58)\n",
    "X_train = []\n",
    "for doc in terms_by_doc_train:\n",
    "    new_txt = []\n",
    "    for word in doc:\n",
    "        try:\n",
    "            new_txt.append(w2indx[word])\n",
    "        except:\n",
    "            new_txt.append(0)\n",
    "    X_train.append(new_txt)\n",
    "X_test = []\n",
    "for doc in terms_by_doc_test:\n",
    "    new_txt = []\n",
    "    for word in doc:\n",
    "        try:\n",
    "            new_txt.append(w2indx[word])\n",
    "        except:\n",
    "            new_txt.append(0)\n",
    "    X_test.append(new_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "(u'X_train shape:', (141900, 10))\n",
      "(u'X_test shape:', (60815, 10))\n",
      "(u'X_train shape:', (141900,))\n",
      "(u'X_test shape:', (60815,))\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "print('X_train shape:',  y_train.shape)\n",
    "print('X_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining a Simple Keras Model...\n",
      "Train on 141900 samples, validate on 60815 samples\n",
      "Epoch 1/20\n",
      " 30720/141900 [=====>........................] - ETA: 24s - loss: 0.9759 - acc: 0.3697"
     ]
    }
   ],
   "source": [
    "print('Defining a Simple Keras Model...')\n",
    "model = Sequential()  # or Graph or whatever\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into vocab_dim dimensions\n",
    "model.add(Embedding(input_dim = n_symbols + 1,\n",
    "                    output_dim = vocab_dim,\n",
    "                    input_length=maxlen,\n",
    "                    ))\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        ))\n",
    "# we use max pooling:\n",
    "\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=20,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "print(\"Evaluate...\")\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history['acc'],label = 'acc train')\n",
    "plt.plot(history.history['val_acc'],label = 'acc val')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Accuarcay\")\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
